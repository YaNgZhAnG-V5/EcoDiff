data:
  save_dir: datasets/sd2/${logger.notes}
  metadata: datasets/gcc3m/Validation_GCC-1.1.0-Validation.tsv
  size: 100
  batch_size: 1

trainer:
  epochs: 3
  beta: 1
  epsilon: 0.
  lr: 2e-2
  attn_lr: 0.2
  ff_lr: 0.2
  n_lr: 0
  model: sd2 # [sd1, sd2, sdxl, sd3]
  device: "cuda:2"
  num_intervention_steps: 50
  seed: 44
  init_lambda: 2.5
  regex: ".*" # "^(down_blocks.[1,2]).*" # optional are ^(down_block)*, ^(up_block)*, .* (for all heads)
  attn_name: attn # use to filter the attention heads, e.g. attn2 only for cross attention
  head_num_filter: 1 # number of heads to filter, apply lambda to the layter that has more than head_num_filter heads
  masking: "hard_discrete" # [sigmoid, hard_discrete]
  masking_eps: 0.5
  use_log: false
  precision: 'bf16' # [fp16, bf16]
  disable_progress_bar: true
  accumulate_grad_batches: 4 #Â batch size
  grad_checkpointing: true

lr_scheduler:
  type: "constant" # [linear, cosine, cosine_with_restarts, constant, polynomial]
  warmup_steps: 10 # helps to start with a lower learning rate
  num_cycles: 1
  power: 1.0
  decay_steps: 0

loss:
  reg: 1 # 2 for L2 norm, 1 for L1 norm, 0 for L0 norm
  reconstruct: 2 # 2 for L2 norm, 1 for L1 norm
  use_attn_reg: true
  use_ffn_reg: true
  mean: true

logger:
  output_dir: "results"
  type: "wandb" # [wandb, csv]
  plot_interval: 8
  project: "sd2_debug_all"
  notes: model_${trainer.model}_eps_${trainer.masking_eps}_sample_${data.size}_beta_${trainer.beta}_epochs_${trainer.epochs}_lr_${trainer.attn_lr}${trainer.ff_lr}_batch_size_${trainer.accumulate_grad_batches}_loss_${loss.reconstruct}${loss.reg}_regex_${trainer.regex}_masking_${trainer.masking}
  tags:
    - "large_blocks_lambda_only"
    - "beta_${trainer.beta}"
    - "recon_norm_${loss.reconstruct}"
    - "reg_norm_${loss.reg}"
  save_lambda_path:
    attn: results/${logger.project}/${logger.notes}/latest_lambda.pt
    ffn: results/${logger.project}/${logger.notes}/latest_lambda.pt

accelerator:
  gradient_accumulation_steps: 1
  mixed_precision: False
  report_to: ${logger.type}

debug: true
debug_cfg:
  data.size: 12
  logger.plot_interval: 4
  trainer.num_intervention_steps: 50
  logger.type: "csv"
